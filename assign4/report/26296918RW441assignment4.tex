\documentclass[10pt, conference]{IEEEtran}

\usepackage{cite}
\usepackage{url}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[font=footnotesize,labelfont=bf]{subcaption}




\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Evaluating the Sensitivity of Isolation Forest Parameters in Anomaly Detection\\

}

\author{\IEEEauthorblockN{DT Nicolay 26296918}
\IEEEauthorblockA{\textit{Computer Science Division} \\
\textit{Stellenbosch University}\\
Stellenbosch, South Africa \\
26296918@sun.ac.za}
}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\begin{IEEEkeywords}
TODO
\end{IEEEkeywords}

\section{Introduction}
% define "anomalies"


\section{Background}
\subsection{Isolation Forests}
% background inituition
% main idea
% algorithm?
The majority of existing model-based approaches to anomaly detection construct a profile of normal instances, then they identify instances that do not conform to this normal profile as anomalies \cite{iforest}. Liu et al. (2008) proposed a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points, Isolation Forests. Here, isolation refers to separating an instance from the rest of the instances. This is ideal for an anomaly detection problem context, since anomalies are by nature sparse and diverse.

% iforest is bettern than statisitcal method, classification-based methods, clustering-based method (they all use normal )
Normal profile methods, since not optimised for anomaly detection, often lead to too many false positives or little to no anomalies detected at all. These methods are also constrained to low dimensional data and small data size since they require significant computational power. Isolation Forests on the other hand take advantage of anomaly datasets consisting of fewer observations for the target class, and anomalies having feature values distinct from the rest of the data. Due to the nature of anomaly observations, they are isolated closer to the root of the tree. This forms the foundation of Isolation Trees.

Isolation Forests involve an ensemble of isolation trees where the predicted anomalies are the observations with the shortest average paths across the trees. Using this model for anomaly detection involves two stages. The first stage constructs isolation trees by recursively partitioning. This training stage is described in Algorithm~\ref{alg:iforest} and Algorithm~\ref{alg:itree} \cite{iforest2}.

\begin{algorithm}[H]
	\caption{iForest($X$, $t$, $\psi$)}
	\label{alg:iforest}
	\begin{algorithmic}[1]
		\Require $X$ -- input data, \ $t$ -- number of trees, \ $\psi$ -- subsampling size
		\Ensure A set of $t$ isolation trees
		\State Initialize $\text{Forest} \gets \emptyset$
		\For{$i = 1$ to $t$}
		\State $X' \gets \text{sample}(X, \psi)$
		\State $\text{Forest} \gets \text{Forest} \cup \text{iTree}(X')$
		\EndFor \\
		\Return $\text{Forest}$
	\end{algorithmic}
\end{algorithm}

The iTree algorithm recursively partitions the dataset by randomly selecting a feature and a random split value until the data can no longer be divided. The resulting tree structure isolates individual points, where anomalies tend to have shorter average path lengths because they are easier to isolate.

\begin{algorithm}[H]
	\caption{iTree($X'$)}
	\label{alg:itree}
	\begin{algorithmic}[1]
		\Require $X'$ -- input data  
		\Ensure an isolation tree
		\If{$X'$ cannot be divided}
		\Return $\text{exNode}\{\text{Size} \leftarrow |X'|\}$
		\Else
		\State Let $Q$ be the list of attributes in $X'$
		\State Randomly select an attribute $q \in Q$
		\State Randomly select a split point $p$ between the max and min values of attribute $q$ in $X'$
		\State $X_l \gets \text{filter}(X', q < p)$
		\State $X_r \gets \text{filter}(X', q \ge p)$
		\Return $\text{inNode}\{\text{Left} \leftarrow \text{iTree}(X_l), \text{Right} \leftarrow \text{iTree}(X_r), \text{SplitAtt} \leftarrow q, \text{SplitValue} \leftarrow p\}$
		\EndIf
	\end{algorithmic}
\end{algorithm}


% TODO add some more commmentary from looking at sklearn docs


\subsection{Control Parameters}
% original paper parms num est & max samples
There are five control parameters to consider namely: the number of estimators, the maximum samples, the contamination, the maximum features, and whether to first bootstrap sample.

The number of trees parameter determines the number of base estimators in the ensemble. This parameter is denoted by $t$ in Algorithm~\ref{alg:iforest}. The performance of Isolation Forests converges quickly with a very small number of trees \cite{iforest}.


The maximum samples describes the number of sample observations to draw from the training data for each base estimator. The subsampling size is denoted by $\varphi$ in Algorithm~\ref{alg:iforest}. Only a small sampling size is required to achieve high detection performance with high efficiency \cite{iforest}. Setting $\varphi$ to 256 often suffices for anomaly detection across a wide range of data \cite{iforest2}.

% contamination additional parameter
Contamination refers to the proportion of anomalies present in the dataset. It is defined as the number of anomalies divided by total number of observations. When set to a specific value between 0 and 0.5, it determines the threshold on the anomaly scores such that approximately that fraction of the training samples are labelled as outliers. In the original paper, the threshold is automatically fixed at an offset of -0.5, following the original Isolation Forest formulation, where inliers typically yield scores near 0 and outliers near -1, allowing the model to separate them without prior knowledge of the true contamination level.

% max features additional parameter
The maximum features describes how many features are selected at random before tree construction to train each base estimator.

% bootstrap additional parameter 
The bootstrap parameter controls the manner in which sample observations are drawn for each tree. This determines whether sampling is done with or without replacement. That is, sub-sample $X'$ is randomly sampled with or without replacement in Algorithm~\ref{alg:iforest} from $X$. Bootstrap resampling can lead to marginal improvements across classification metrics \cite{Choi2025Impact}.

\subsection{Evaluation Stage}


\section{Implementation}
\subsection{test}

\section{Empirical Process}



\section{Results \& Discussion}



\section{Conclusions}


\bibliographystyle{IEEEtran}
\bibliography{references}






\end{document}
