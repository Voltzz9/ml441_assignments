\documentclass[10pt, conference]{IEEEtran}

\usepackage{cite}
\usepackage{url}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[font=footnotesize,labelfont=bf]{subcaption}




\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Evaluating the Sensitivity of Isolation Forest Parameters in Anomaly Detection\\

}

\author{\IEEEauthorblockN{DT Nicolay 26296918}
\IEEEauthorblockA{\textit{Computer Science Division} \\
\textit{Stellenbosch University}\\
Stellenbosch, South Africa \\
26296918@sun.ac.za}
}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\begin{IEEEkeywords}
TODO
\end{IEEEkeywords}

\section{Introduction}
% Start with purpose


% then what was done

% define "anomalies"

% main finding

% report structure description

\section{Background}
\subsection{Isolation Forests}
% background inituition
% main idea
% algorithm?
The majority of existing model-based approaches to anomaly detection construct a profile of normal instances, then they identify instances that do not conform to this normal profile as anomalies \cite{iforest}. Liu et al. (2008) proposed a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points, Isolation Forests. Here, isolation refers to separating an instance from the rest of the instances. This is ideal for an anomaly detection problem context, since anomalies are by nature sparse and diverse.

% iforest is bettern than statisitcal method, classification-based methods, clustering-based method (they all use normal )
Normal profile methods, since not optimised for anomaly detection, often lead to too many false positives or little to no anomalies detected at all. These methods are also constrained to low dimensional data and small data size since they require significant computational power. Isolation Forests on the other hand take advantage of anomaly datasets consisting of fewer observations for the target class, and anomalies having feature values distinct from the rest of the data. Due to the nature of anomaly observations, they are isolated closer to the root of the tree. This forms the foundation of Isolation Trees.

Isolation Forests involve an ensemble of isolation trees where the predicted anomalies are the observations with the shortest average paths across the trees. Using this model for anomaly detection involves two stages. The first stage constructs isolation trees by recursively partitioning. This training stage is described in Algorithm~\ref{alg:iforest} and Algorithm~\ref{alg:itree} \cite{iforest2}.

\begin{algorithm}[H]
	\caption{iForest($X$, $t$, $\psi$)}
	\label{alg:iforest}
	\begin{algorithmic}[1]
		\Require $X$ -- input data, \ $t$ -- number of trees, \ $\psi$ -- subsampling size
		\Ensure A set of $t$ isolation trees
		\State Initialize $\text{Forest} \gets \emptyset$
		\For{$i = 1$ to $t$}
		\State $X' \gets \text{sample}(X, \psi)$
		\State $\text{Forest} \gets \text{Forest} \cup \text{iTree}(X')$
		\EndFor \\
		\Return $\text{Forest}$
	\end{algorithmic}
\end{algorithm}

The iTree algorithm recursively partitions the dataset by randomly selecting a feature and a random split value until the data can no longer be divided. The resulting tree structure isolates individual points, where anomalies tend to have shorter average path lengths because they are easier to isolate.

\begin{algorithm}[H]
	\caption{iTree($X'$)}
	\label{alg:itree}
	\begin{algorithmic}[1]
		\Require $X'$ -- input data  
		\Ensure an isolation tree
		\If{$X'$ cannot be divided}
		\Return $\text{exNode}\{\text{Size} \leftarrow |X'|\}$
		\Else
		\State Let $Q$ be the list of attributes in $X'$
		\State Randomly select an attribute $q \in Q$
		\State Randomly select a split point $p$ between the max and min values of attribute $q$ in $X'$
		\State $X_l \gets \text{filter}(X', q < p)$
		\State $X_r \gets \text{filter}(X', q \ge p)$
		\Return $\text{inNode}\{$ \\
		\hspace*{1.5em} $\text{Left} \leftarrow \text{iTree}(X_l),$ \\
		\hspace*{1.5em} $\text{Right} \leftarrow \text{iTree}(X_r),$ \\
		\hspace*{1.5em} $\text{SplitAtt} \leftarrow q,$ \\
		\hspace*{1.5em} $\text{SplitValue} \leftarrow p\}$ 
		\EndIf
	\end{algorithmic}
\end{algorithm}


% TODO add some more commmentary from looking at sklearn docs


\subsection{Control Parameters}
% original paper parms num est & max samples
There are five control parameters to consider namely: the number of estimators, the maximum samples, the contamination, the maximum features, and whether to first bootstrap sample.

The number of trees parameter determines the number of base estimators in the ensemble. This parameter is denoted by $t$ in Algorithm~\ref{alg:iforest}. The performance of Isolation Forests converges quickly with a very small number of trees \cite{iforest}.


The maximum samples describes the number of sample observations to draw from the training data for each base estimator. The subsampling size is denoted by $\psi$ in Algorithm~\ref{alg:iforest}. Only a small sampling size is required to achieve high detection performance with high efficiency \cite{iforest}. Setting $\psi$ to 256 often suffices for anomaly detection across a wide range of data \cite{iforest2}.

% contamination additional parameter
Contamination refers to the proportion of anomalies present in the dataset. It is defined as the number of anomalies divided by total number of observations. When set to a specific value between 0 and 0.5, it determines the threshold on the anomaly scores such that approximately that fraction of the training samples are labelled as outliers. In the original paper, the threshold is automatically fixed at an offset of -0.5, following the original Isolation Forest formulation, where inliers typically yield scores near 0 and outliers near -1, allowing the model to separate them without prior knowledge of the true contamination level. Since this parameter directly influences the threshold for anomaly detection, mis-tuning will directly increase false positives or false negative rates.

% max features additional parameter
The maximum features describes how many features are selected at random before tree construction to train each base estimator. Introducing randomness can help with with high-dimensional data, but may increase instability if set too low.

% bootstrap additional parameter 
The bootstrap parameter controls the manner in which sample observations are drawn for each tree. This determines whether sampling is done with or without replacement. That is, sub-sample $X'$ is randomly sampled with or without replacement in Algorithm~\ref{alg:iforest} from $X$. Bootstrap resampling can lead to marginal improvements across classification metrics \cite{Choi2025Impact}.

\subsection{Evaluation Stage}
For this stage, a single path length $h(\boldsymbol{x})$ is computed by counting the number of edges from the root node to an external node as an instance $\boldsymbol{x}$ traverse each iTree. There is a predefined height limit, \textit{hlim}, that when reached, the algorithm returns the value $e$ plus an adjustment $c(Size)$. This process is described in Algorithm~\ref{alg:pathlength} \cite{iforest2}. The worst case time complexity of the evaluation stage for a dataset size $n$ is $O(nt \psi)$.

\begin{algorithm}[H]
	\caption{PathLength($\boldsymbol{x}, T, h_{lim}, e$)}
	\label{alg:pathlength}
	\begin{algorithmic}[1]
		\Require $\boldsymbol{x}$ -- an instance; $T$ -- an iTree; $h_{lim}$ -- height limit; $e$ -- current path length (initialized to 0 when first called)
		\Ensure Path length of $\boldsymbol{x}$
		\If{$T$ is an external node \textbf{or} $e \ge h_{lim}$}
		\State \Return $e + c(T.\text{size})$ \Comment{$c(\cdot)$ is defined in Equation (1)}
		\Else
		\State $a \gets T.\text{splitAtt}$
		\If{$x_a < T.\text{splitValue}$}
		\State \Return $\text{PathLength}(\boldsymbol{x}, T.\text{left}, h_{lim}, e + 1)$
		\Else
		\State \Return $\text{PathLength}(\boldsymbol{x}, T.\text{right}, h_{lim}, e + 1)$
		\EndIf
		\EndIf
	\end{algorithmic}
\end{algorithm}

The anomaly score $s(\boldsymbol{x}, \psi)$ for instance $\boldsymbol{x}$ is computed using

\begin{equation}
	s(\boldsymbol{x} \psi) = 2^{- \frac{E(h(\boldsymbol{x}))}{c(\psi)}},
\end{equation}

where $c(\psi)$ is the average path length of unsuccessful searches and $E(h(\boldsymbol{x}))$ is the average depth from the forest of isolation trees. If the score of an observation $s$ is close to 1, the observation is an anomaly. If an observation has a score much smaller than 0.5, then it is regarded as a normal instance. However, if all observations return a score of approximately 0.5, then there are no distinct anomalies.


\section{Implementation}
\subsection{Isolation Forest Algorithm}
% brief recap alg
The Scikit-learn library \cite{scikit-learn} implementation of the Isolation Forest algorithm described in the background section is used to detect anomalies across three varied datasets. 

A random seed of 12 is used for all stochastic procedures to ensure reproducibility. All tests are conducted on a Fedora Linux 41 system with a 13th Gen Intel Core i3-13100 CPU with 16 GiB of memory. The computationally significant portions of the algorithm are running the algorithm with a large ensemble size and also a large parameter grid for some parameter combinations. The code for this paper is available on GitHub \cite{github}.

\subsection{Control Parameters}
% explain the ranges tested over
\section{Empirical Process}
% 20 MARKS
\subsection{Datasets}
% table summary
% rationale for selection
\subsection{Evaluation Metrics}
% metrics and justification

\subsection{Experimental Design}
\subsubsection{Single-Parameter Analysis}
%Methodology: Vary one parameter while holding others at defaults
%Repetitions: Number of runs per configuration (e.g., 10 runs for stability)
%Baseline configuration: Specify default parameter set
\subsubsection{Multi-Parameter Interaction Analysis}
% grid search specify values
% vis strategy
\subsubsection{Cross-Dataset Comparison}
%consistency
%identify data depepedent properties

\subsection{Analysis Framework}
% convergence analysis

% subsampling anayside

%contamination analysis

% feature subsampling imapact

% bootstrap

\subsection{Reproducibility}
% random seed
% link to github



\section{Results \& Discussion}
% intro
\subsection{Number of Estimators}
\subsubsection{Performance Convergence}
% show multi plot

% refer to appendix

% observations - where do we plataeu

% dataset complexity and reqiured ensemble size

\subsubsection{Stability and Variance Reduction}
% coef variation vs n est

% observe exponetial decay in variance?

% Law of large numbers in ensemble context

% stability threshold

\subsubsection{Computational Efficiency}
% f1 vs training time scatter

% sweet spot observe

%give practical recommendation

% when to use larger

\subsection{Maximum Samples}
\subsubsection{Performance vs. Subsample size}
% fig f1 all cs max samples

% observe is auto optimal

% data specific patterns

% discuss bias variance tradoff

%Large samples → swamping (anomalies hidden in normal data)

\subsubsection{Swamping Effect Analysis}
% fig precision recall curves

% observe large samples boos recal but hurt precision, small sames opposive

% when does swamping become bad

% mechanism: small samples isolate anomalies faster

\subsubsection{Training Time Complexity}
% fig

% is siblinear?

% comp advatnages?

\subsubsection{Emperical Rule Derivation}
% fig



\subsection{Contamination Parameter}
\subsubsection{Robustness to Misspecification}
% fig: perfromance - heatmap or line plot

% over or under

% why IF robust to errors?

% practical overestinamation?  - some cases want overestimate like fraud

\subsubsection{Precision-Recall Trade-off}
% fig: stacked bar / dual axis 
%igh contamination → high recall, low precision
%Trade-off quantification
%
%
%Discussion:
%
%Mechanism: threshold adjustment via contamination
%Application-specific tuning (cost of false positives vs. negatives)

\subsubsection{Calibration Analysis}
% idk have a closer look


\subsection{Maximum Features}
\subsubsection{Performance}
% fig: f1 ... vs max featu

% observe high dim improve

% low dim degredation?

% curse of dimensionality miitgation

% irrelevant feature noise reduction

% when full features are necessary


\subsubsection{Efficiency-Performance Trade-off}
% fig: dual axis plot f1 and time v max feat

% observe sweet spot and data specific patterns

% give practical recommendation for high dim data, mention diminishing returns

\subsubsection{Interaction with Ensemble Size}
% fig: speech only heatmap of f1 score

% obseve can more trees compensate for fewer features?

% trade ioff

% practical guidance

\subsection{Bootstrap Parameter}
\subsubsection{Stability Comparison}
% fig: box plots with and without bootstrap

% statistical wilcoxon signed rank

% effect size cohen's d annotation

% variance differences across datasets

% statistical sig

\subsubsection{Performance Metrics}
% give table of performance

% say how it doesnt help much

\subsubsection{Interaction with Contamination}
% fig: grouped bar

%Does bootstrap effect depend on contamination level?

\subsection{Bootstrap}

\subsection{Practical Guidelines}
% table with guide
\section{Conclusions}


\bibliographystyle{IEEEtran}
\bibliography{references}






\end{document}
